"""classification_engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W6PvKwVRTX-EjnXPcY75OinemKrDOrq5

Import necessary modules and functions
"""

# !pip install langchain-google-genai google-generativeai
# !pip install google-auth-oauthlib
# !pip install langchain-community
# !pip install pandas
# !pip install requests

import os
import re
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum

from logger import logger

# # Load environment variables
# load_dotenv()


class DocumentType(Enum):
    ENQUIRY = "Enquiry"
    QUOTATION = "Quotation"
    UNKNOWN = "Unknown"


@dataclass
class ClassificationResult:
    """
    Enhanced classification result with comprehensive metadata for archival.

    Attributes:
        document_type: Type of document (ENQUIRY/QUOTATION/UNKNOWN)
        confidence_score: Classification confidence (0.0-1.0)
        found_sections: List of identified document sections
        missing_sections: List of expected but missing sections
        reasoning: Detailed explanation of classification decision
        keyword_matches: Dictionary of matched keywords by category
        bank_details_found: Whether bank details were detected

        # Archive Metadata
        processing_timestamp: When the classification was performed
        version: Classification engine version
        model_info: Information about the AI model used
        validation_status: Document validation status
        quality_metrics: Dictionary of quality assessment metrics
    """

    # Core Classification
    document_type: DocumentType
    confidence_score: float
    found_sections: list[str]
    missing_sections: list[str]
    reasoning: str
    keyword_matches: dict[str, list[str]]
    bank_details_found: bool = False

    # Archive Metadata
    processing_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    version: str = "1.0.0"
    model_info: dict[str, str] = field(
        default_factory=lambda: {"name": "classification_engine", "version": "1.0.0", "last_updated": "2025-06-23"}
    )
    validation_status: str = "pending"
    quality_metrics: dict[str, float] = field(
        default_factory=lambda: {"completeness": 0.0, "consistency": 0.0, "accuracy": 0.0}
    )


class DocumentClassifier:
    """
    Enhanced document classifier with improved section detection, keyword matching, and bank details recognition
    """

    def __init__(self):
        # Enhanced section patterns for enquiries
        self.enquiry_sections = [
            "company details",
            "site information",
            "scope of supply",
            "scope of work",
            "application details",
            "raw material details",
            "extruder details",
            "project details",
            "technical specifications",
            "machinery details",
            "equipment details",
            "capacity requirements",
            "production requirements",
        ]

        # Enhanced section patterns for quotations
        self.quotation_sections = [
            "gst number",
            "quotation number",
            "payment terms",
            "payment details",
            "bank details",
            "bank information",
            "tax details",
            "terms conditions",
            "delivery terms",
            "validity",
            "price amount",
            "total",
            "subtotal",
            "commercial terms",
            "financial terms",
        ]

        # Enhanced section synonym groups for normalization
        self.section_synonyms = {
            # GST related terms
            "gstin": "gst number",
            "gstn": "gst number",
            "gst": "gst number",
            "gst no": "gst number",
            "gst number": "gst number",
            "tax identification": "gst number",
            # Quotation number related terms
            "quotation no": "quotation number",
            "quotation number": "quotation number",
            "quote no": "quotation number",
            "quote number": "quotation number",
            "quotation ref": "quotation number",
            "quote ref": "quotation number",
            "reference number": "quotation number",
            # Payment related terms
            "payment details": "payment details",
            "payment terms": "payment terms",
            "payment conditions": "payment terms",
            "payment method": "payment terms",
            "payment information": "payment details",
            # Bank details (NEW)
            "bank details": "bank details",
            "bank information": "bank details",
            "banking details": "bank details",
            "bank account details": "bank details",
            "account details": "bank details",
            "transfer details": "bank details",
            "wire transfer details": "bank details",
            # Terms and conditions
            "terms & conditions": "terms conditions",
            "terms and conditions": "terms conditions",
            "terms conditions": "terms conditions",
            "t&c": "terms conditions",
            "terms & condition": "terms conditions",
            "conditions": "terms conditions",
            # Price and amount (ENHANCED)
            "price": "price amount",
            "amount": "price amount",
            "cost": "price amount",
            "rate": "price amount",
            "total basic price": "price amount",  # NEW
            "basic price": "price amount",  # NEW
            "total price": "price amount",  # NEW
            "final amount": "price amount",  # NEW
            "grand total": "total",
            # Delivery terms
            "delivery terms": "delivery terms",
            "delivery conditions": "delivery terms",
            "shipping terms": "delivery terms",
            "delivery schedule": "delivery terms",
        }

        # Enhanced keyword patterns
        self.enquiry_keywords = [
            "enquiry",
            "inquiry",
            "requirement",
            "specification",
            "tender",
            "rfq",
            "request for quotation",
            "technical details",
            "capacity",
            "production",
            "manufacturing",
            "machinery",
            "equipment",
            "raw material",
            "application",
            "scope of work",
            "project",
            "technical specification",
            "machine requirement",
            "equipment need",
        ]

        self.quotation_keywords = [
            "quotation",
            "quote",
            "price",
            "cost",
            "amount",
            "payment",
            "terms",
            "conditions",
            "validity",
            "delivery",
            "tax",
            "gst",
            "total",
            "subtotal",
            "discount",
            "offer",
            "proposal",
            "commercial",
            "financial",
            "bank",
            "account",
            "transfer",
            "basic price",
            "final price",  # NEW
        ]

        # Bank details patterns for enhanced detection
        self.bank_detail_patterns = [
            r"bank\s+name",
            r"bank\s+address",
            r"account\s+name",
            r"account\s+number",
            r"account\s+type",
            r"micr\s+code",
            r"ifsc\s+code",
            r"rtgs\s+code",
            r"swift\s+code",
            r"routing\s+number",
            r"sort\s+code",
            r"branch\s+code",
        ]

    def extract_sections_from_markdown(self, content: str) -> list[str]:
        """
        Enhanced section extraction with multiple patterns and better cleaning
        """
        sections = set()

        # Enhanced patterns for section detection
        header_patterns = [
            r"^#{1,6}\s+(.+?)(?:\s*\{[^}]*\})?$",
            r"^\*\*(.+?)\*\*\s*:?\s*$",
            r"^__(.+?)__\s*:?\s*$",
            r"^(.+?):\s*$",
            r"^\d+\.\s*(.+?):\s*$",
            r"^\d+\)\s*(.+?):\s*$",
            r"^[-*+]\s*(.+?):\s*$",
            r"^([A-Z][A-Z\s&]+[A-Z]):\s*$",
            r"^\|\s*(.+?)\s*\|",
            r"^(.+?)\s*[-=]{3,}\s*$",  # Underlined headers
        ]

        lines = content.split("\n")
        for line in lines:
            line = line.strip()
            if not line or len(line) < 3:
                continue

            for pattern in header_patterns:
                match = re.match(pattern, line, re.IGNORECASE | re.MULTILINE)
                if match:
                    section_text = match.group(1).strip()
                    section_text = re.sub(r"[*_`#|]", "", section_text)
                    section_text = re.sub(r"\s+", " ", section_text)
                    if len(section_text) > 2:
                        sections.add(section_text.lower())
                    break

        return list(sections)

    def detect_bank_details(self, content: str) -> bool:
        """
        Enhanced bank details detection using multiple patterns
        """
        content_lower = content.lower()

        # Count matches for bank detail patterns
        matches = 0
        for pattern in self.bank_detail_patterns:
            if re.search(pattern, content_lower, re.IGNORECASE):
                matches += 1

        # Also look for common bank detail indicators
        bank_indicators = [
            "bank details for transferring payment",
            "bank information",
            "account number",
            "ifsc code",
            "swift code",
            "micr code",
            "account name",
            "bank name",
        ]

        for indicator in bank_indicators:
            if indicator in content_lower:
                matches += 1

        # Return True if we found at least 2 bank-related patterns
        return matches >= 2

    def extract_keywords(self, content: str) -> set[str]:
        """
        Extract relevant keywords from the document content
        """
        content_lower = content.lower()
        clean_content = re.sub(r"[*_`#|]", "", content_lower)
        clean_content = re.sub(r"\s+", " ", clean_content)

        # Extract words and phrases
        words = set(re.findall(r"\b[a-zA-Z0-9]+\b", clean_content))

        # Also extract common multi-word phrases
        phrases = set()
        for keyword in self.enquiry_keywords + self.quotation_keywords:
            if keyword.lower() in clean_content:
                phrases.add(keyword)

        return words.union(phrases)

    def calculate_keyword_matches(self, content_words: set[str], target_keywords: list[str]) -> tuple[list[str], float]:
        """
        Calculate keyword matches with partial matching and scoring
        """
        matched_keywords = []
        total_score = 0

        for keyword in target_keywords:
            keyword_lower = keyword.lower()

            # Direct match
            if keyword_lower in content_words:
                matched_keywords.append(keyword)
                total_score += 1.0
            # Partial match for multi-word keywords
            elif " " in keyword_lower:
                keyword_words = keyword_lower.split()
                matches = sum(1 for word in keyword_words if word in content_words)
                if matches > 0:
                    partial_score = matches / len(keyword_words)
                    if partial_score >= 0.5:
                        matched_keywords.append(keyword)
                        total_score += partial_score

        if not target_keywords:
            return matched_keywords, 0.0

        confidence = min(total_score / len(target_keywords), 1.0)
        return matched_keywords, confidence

    def normalize_section(self, section: str) -> str:
        """
        Normalize section names using enhanced synonym mapping
        """
        section_lower = section.lower().strip()

        # Check for exact matches first
        if section_lower in self.section_synonyms:
            return self.section_synonyms[section_lower]

        # Check for partial matches
        for synonym, normalized in self.section_synonyms.items():
            if synonym in section_lower or section_lower in synonym:
                return normalized

        return section_lower

    def _section_match_score(self, found: str, target: str) -> float:
        """
        Helper to calculate the match score between a found section and a target section.
        """
        found_lower = found.lower()
        target_lower = target.lower()

        # Exact match after normalization
        if target_lower == found_lower:
            return 1.0
        # Substring match
        elif target_lower in found_lower or found_lower in target_lower:
            similarity = min(len(target_lower), len(found_lower)) / max(len(target_lower), len(found_lower))
            return similarity * 0.8
        # Keyword overlap
        else:
            target_words = set(target_lower.split())
            found_words = set(found_lower.split())
            common_words = target_words.intersection(found_words)
            if common_words:
                overlap_score = len(common_words) / max(len(target_words), len(found_words))
                if overlap_score >= 0.6:
                    return overlap_score * 0.6
        return 0.0

    def calculate_section_matches(
        self, found_sections: list[str], target_sections: list[str]
    ) -> tuple[list[str], float]:
        """
        Enhanced section matching with normalization and fuzzy matching
        """
        matched_sections = []
        total_score = 0

        normalized_found = [self.normalize_section(section) for section in found_sections]

        for target in target_sections:
            best_match_score = 0
            for found in normalized_found:
                score = self._section_match_score(found, target)
                if score > 0:
                    if target not in matched_sections:
                        matched_sections.append(target)
                    best_match_score = max(best_match_score, score)
                    if score == 1.0:
                        break  # Early exit on perfect match
            total_score += best_match_score

        if not target_sections:
            return matched_sections, 0.0

        confidence = total_score / len(target_sections)
        return matched_sections, confidence

    def classify_document(self, content: str) -> ClassificationResult:
        """
        Enhanced document classification with bank details detection
        """
        try:
            logger.info("Classifying document")
            # Extract sections and keywords
            found_sections = self.extract_sections_from_markdown(content)
            content_words = self.extract_keywords(content)

            # Detect bank details
            bank_details_detected = self.detect_bank_details(content)

            # Calculate section matches
            enquiry_section_matches, enquiry_section_confidence = self.calculate_section_matches(
                found_sections, self.enquiry_sections
            )

            quotation_section_matches, quotation_section_confidence = self.calculate_section_matches(
                found_sections, self.quotation_sections
            )

            # Calculate keyword matches
            enquiry_keyword_matches, enquiry_keyword_confidence = self.calculate_keyword_matches(
                content_words, self.enquiry_keywords
            )

            quotation_keyword_matches, quotation_keyword_confidence = self.calculate_keyword_matches(
                content_words, self.quotation_keywords
            )

            # Bank details boost for quotations
            bank_boost = 0.2 if bank_details_detected else 0.0

            # Combined scoring with bank details consideration
            enquiry_combined_score = (enquiry_section_confidence * 0.6) + (enquiry_keyword_confidence * 0.4)
            quotation_combined_score = (
                (quotation_section_confidence * 0.6) + (quotation_keyword_confidence * 0.4) + bank_boost
            )

            # Determine document type
            min_confidence_threshold = 0.2
            significant_difference = 0.15

            keyword_matches = {
                "enquiry_keywords": enquiry_keyword_matches,
                "quotation_keywords": quotation_keyword_matches,
            }

            if (
                enquiry_combined_score > quotation_combined_score + significant_difference
                and enquiry_combined_score > min_confidence_threshold
            ):
                doc_type = DocumentType.ENQUIRY
                confidence = enquiry_combined_score
                matched = enquiry_section_matches
                missing = [s for s in self.enquiry_sections if s not in enquiry_section_matches]
                reasoning = (
                    f"Classified as Enquiry with {confidence:.2%} confidence. "
                    f"Section score: {enquiry_section_confidence:.2%}, "
                    f"Keyword score: {enquiry_keyword_confidence:.2%}. "
                    f"Found {len(matched)} relevant sections and {len(enquiry_keyword_matches)} keywords."
                )

            elif (
                quotation_combined_score > enquiry_combined_score + significant_difference
                and quotation_combined_score > min_confidence_threshold
            ):
                doc_type = DocumentType.QUOTATION
                confidence = quotation_combined_score
                matched = quotation_section_matches
                missing = [s for s in self.quotation_sections if s not in quotation_section_matches]
                reasoning = (
                    f"Classified as Quotation with {confidence:.2%} confidence. "
                    f"Section score: {quotation_section_confidence:.2%}, "
                    f"Keyword score: {quotation_keyword_confidence:.2%}. "
                    f"Bank details detected: {bank_details_detected}. "
                    f"Found {len(matched)} relevant sections and {len(quotation_keyword_matches)} keywords."
                )

            else:
                doc_type = DocumentType.UNKNOWN
                confidence = max(enquiry_combined_score, quotation_combined_score)
                matched = (
                    enquiry_section_matches
                    if enquiry_combined_score > quotation_combined_score
                    else quotation_section_matches
                )
                missing = []
                reasoning = (
                    f"Could not classify with confidence. "
                    f"Enquiry score: {enquiry_combined_score:.2%}, "
                    f"Quotation score: {quotation_combined_score:.2%}. "
                    f"Bank details detected: {bank_details_detected}. "
                    f"Difference too small or scores too low. Requires manual review."
                )

            return ClassificationResult(
                document_type=doc_type,
                confidence_score=confidence,
                found_sections=matched,
                missing_sections=missing,
                reasoning=reasoning,
                keyword_matches=keyword_matches,
                bank_details_found=bank_details_detected,
            )

        except Exception as e:
            logger.error("Error classifying document: %s", str(e))
            return "UNRELATED"


# Initialize the enhanced classifier
classifier = DocumentClassifier()


# Clean example usage function
def classify_document_clean(file_path: str) -> str:
    """
    Clean function to classify a document and return only essential information
    """
    try:
        # Handle directory path by looking for output_all_pages.md
        if os.path.isdir(file_path):
            file_path = os.path.join(file_path, "output_all_pages.md")

        # Check if file exists
        if not os.path.exists(file_path):
            return f"Error: File not found at path: {file_path}"

        with open(file_path, encoding="utf-8") as file:
            content = file.read()

        if not content.strip():
            return "Error: The file appears to be empty"

        result = classifier.classify_document(content)

        # Return only the essential information
        return f"{result.document_type.value}, {result.confidence_score:.2%}"

    except Exception as e:
        logger.error("Error processing file classify_document_clean : %s", str(e))
        return f"Error processing file: {str(e)}"


# Example usage with clean output
# if __name__ == "__main__":
#     # Test with your example files
#     path_file = (
#         """/mnt/c/Users/Imran/OneDrive - Ahana Systems and Solutions (P) Ltd/Desktop/Demo/
# steer_document_processing_poc/demo_app/backend/utils/image
# /6c44bc93-7c76-47a2-a9c1-c750f228c583_TSE_Enquiry_form-Mega_80S
# /output_all_pages.md"""
#     )
#     path_file = (
#         "/mnt/c/Users/Imran/OneDrive - Ahana Systems and Solutions (P) Ltd/Desktop/Demo/
# steer_document_processing_poc/demo_app/backend/utils/image/
# 0ac8d2a5-c066-4c26-81fa-8d30f9a4713d_QU-IMM-Vi-42025-00169-1-28-04-2025-STEER_ENGGREV/output_all_pages.md"
#     )

#     try:
#         x=classify_document_clean(path_file)

#         print(x.split(',')[0].strip())
#         print(x.split(',')[1].strip())
#         #if 'Enquiry'==x.split(',')[0].strip():
#             # path='
#         # elif 'Quotation' == x.split(',')[0].strip():


#         new_data=x.split(',')[0].strip()


#     except Exception as e:
#         print(f"Error: {e}")
